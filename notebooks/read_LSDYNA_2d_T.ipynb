{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766cfd39-b1a9-4e7a-b0f5-c5d152c16f7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parse LSDYNA file to extract particle coordinate, type, Von Mises Stree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4f7237-0e16-4ce3-8936-e2edc7b64941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "LOADING_PARTICLES = []\n",
    "SUPPORT_PARTICLES = []\n",
    "\n",
    "def parse_simulation(file):\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find all \"particle position\" lines and \"plastic strain\" lines using key words\n",
    "    pos_lines_start, pos_lines_end = [], []\n",
    "    strain_lines_start, strain_lines_end = [], []\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.startswith(\"*NODE\"):\n",
    "            pos_lines_start.append(idx)\n",
    "        elif line.startswith(\"$NODAL_RESULTS\"):  # $NODAL_RESULTS,(1d) *INITIAL_VELOCITY_NODE(2d)\n",
    "            pos_lines_end.append(idx)\n",
    "        elif line.startswith(\"$RESULT OF Effective Stress (v-m)\"):\n",
    "            strain_lines_start.append(idx)\n",
    "        elif line.startswith(\"*END\"):  \n",
    "            strain_lines_end.append(idx)\n",
    "            \n",
    "    # Extact particle positions \n",
    "    trajectory = []\n",
    "    for line_start, line_end in zip(pos_lines_start, pos_lines_end):\n",
    "        pos_lines = lines[line_start+1:line_end]   # lines that contains positions in one time step\n",
    "        timestep = []\n",
    "        for line in pos_lines:\n",
    "            num_str = re.findall(r'[-\\d\\.e+]+', line)  # Regular expression findign scitific numbers\n",
    "            (x, y) = (float(num_str[1]), float(num_str[2]))\n",
    "            timestep.append((x,y))\n",
    "        trajectory.append(timestep) \n",
    "    \n",
    "    # Extact particle types\n",
    "    particle_types = []\n",
    "    pos_lines = lines[pos_lines_start[0]+1:pos_lines_end[0]]\n",
    "    for line in pos_lines:\n",
    "        num_str = re.findall(r'[-\\d\\.e+]+', line)\n",
    "        if int(num_str[0]) in LOADING_PARTICLES:\n",
    "            particle_types.append(3)   # kinematic particles\n",
    "        elif int(num_str[0]) in SUPPORT_PARTICLES:\n",
    "            particle_types.append(2)   # boundary particles (rigid)\n",
    "        else:\n",
    "            particle_types.append(1)   # normal concrete particles\n",
    "    \n",
    "    # Extrac Von Mises Stress\n",
    "    strains = []\n",
    "    for line_start, line_end in zip(strain_lines_start, strain_lines_end):\n",
    "        strain_lines = lines[line_start+1:line_end]   # lines that contains positions in one time step\n",
    "        strains_one_step = []\n",
    "        for line in strain_lines:\n",
    "            num_str = re.findall(r'[-+\\d\\.Ee]+', line)  # the expression matches one or more repetitions of \"-\", \"integer\", \".\", \"E\",\n",
    "            num = float(num_str[1])\n",
    "            strains_one_step.append(num)\n",
    "        strains.append(strains_one_step)     \n",
    "    \n",
    "\n",
    "    return np.array(trajectory).astype(np.float), np.array(particle_types).astype(np.float), np.array(strains).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75638c53-e4aa-414b-b6df-8524d73ca0a5",
   "metadata": {},
   "source": [
    "# Read Text and write to individual npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8d380-0896-4861-8289-ba7c646b8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "in_dir = f'/home/jovyan/share/EPIMETHEUS-LOCAL/gns_data/Concrete2D-T/LS-DYNA-Output/'\n",
    "out_dir = f'/home/jovyan/share/EPIMETHEUS-LOCAL/gns_data/Concrete2D-T/LS-DYNA-Output/npz'\n",
    "\n",
    "# Create out_dir if it does not exist\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "simulations = glob.glob(in_dir + '*')\n",
    "random.shuffle(simulations)\n",
    "simulations.sort()\n",
    "\n",
    "for idx, simulation in enumerate(simulations):\n",
    "    print(f\"{idx} Reading {simulation}...\")\n",
    "    \n",
    "    trajectory_name = simulation.split('/')[-1]\n",
    "    \n",
    "    # Parse simulation data (ensure this function is defined)\n",
    "    positions, particle_types, strains = parse_simulation(simulation)\n",
    "\n",
    "    # Define output file path\n",
    "    npz_path = os.path.join(out_dir, f\"{trajectory_name}.npz\")\n",
    "\n",
    "    # Save the NumPy arrays in an .npz file\n",
    "    np.savez(npz_path, positions=positions, particle_types=particle_types, strains=strains)\n",
    "\n",
    "    print(f\"Saved {npz_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e266cd-aea1-48cd-9e76-ef9ccbdbbb45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-process and write to npz for GNN training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a50bd9-d350-4cb8-8679-29bcd6379ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Varianbles to be specified\n",
    "DATASET = 'Concrete2D-T-Step2'\n",
    "STEP_SIZE = 2 # downsample the timestep from the numerical model\n",
    "TOTAL_STEP = 100\n",
    "\n",
    "val_set = ['60-120', '80-140', '100-180']\n",
    "test_set = ['60-130', '80-150', '100-170']\n",
    "\n",
    "# Input and Output directory\n",
    "in_dir = f'/home/jovyan/share/EPIMETHEUS-LOCAL/gns_data/Concrete2D-T/npz/'\n",
    "out_dir = f'/home/jovyan/share/EPIMETHEUS-LOCAL/gns_data/{DATASET}/'\n",
    "\n",
    "# Create out_dir if it does not exist\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "## Normalisation parameters\n",
    "pos_max, pos_min = np.array([100, 50]), np.array([-2.5, -50])\n",
    "#strain_mean, strain_std = 143.09920564186177, 86.05175002337249  # Step 1, vms stats pre-computed from data\n",
    "strain_mean, strain_std = 150.25897834554772, 83.50737010164767  # Step 2\n",
    "\n",
    "simulations = glob.glob(in_dir + '*')\n",
    "random.shuffle(simulations)\n",
    "simulations.sort()\n",
    "ds_train, ds_valid, ds_test = {}, {}, {}\n",
    "vels = np.array([]).reshape(0, 2)\n",
    "accs = np.array([]).reshape(0, 2)\n",
    "strain_stats = np.array([])\n",
    "train_info, valid_info, test_info = [], [], []\n",
    "\n",
    "for idx, simulation in enumerate(simulations):\n",
    "    print(f\"{idx} Reading {simulation}...\")\n",
    "    \n",
    "    trajectory_name = simulation.split('/')[-1]\n",
    "    \n",
    "    # Load data from npz\n",
    "    with np.load(simulation) as data:\n",
    "        positions = data['positions']\n",
    "        particle_types = data['particle_types']\n",
    "        strains = data['strains']\n",
    "       \n",
    "    # Identify the time instance when structure response initialise\n",
    "    mean_strain = strains.mean(axis=1)\n",
    "    first_nonzero_step_idx = next((i for i, x in enumerate(mean_strain) if x), None) # x!= 0 for strict match\n",
    "    \n",
    "    # Preprocessing positions\n",
    "    positions = positions[first_nonzero_step_idx-1:first_nonzero_step_idx-1+TOTAL_STEP:STEP_SIZE,:-4:1,:] \n",
    "    positions = (positions - pos_min) / (pos_max - pos_min)  # Normalize based on overall min and max of all simulations\n",
    "    # y_scalling_factor = (pos_max - pos_min)[0] / (pos_max - pos_min)[1]\n",
    "    # positions[:,:,1] = positions[:,:,1] / y_scalling_factor   \n",
    "    \n",
    "    # Change the last 4 particle to boundary particle\n",
    "    particle_types = particle_types[:-4]\n",
    "\n",
    "    # Preprocessing strains\n",
    "    strains = strains[first_nonzero_step_idx-1:first_nonzero_step_idx-1+TOTAL_STEP:STEP_SIZE, :-4:1]\n",
    "    strains = (strains - strain_mean) / strain_std   ## standardize based on overall mean and std\n",
    "    # strain_stats = np.concatenate((strain_stats, strains.flatten()), axis=0) # debug stats\n",
    "    \n",
    "    print(f\"Position min:{positions.min(axis=(0,1))}, max:{positions.max(axis=(0,1))}\")\n",
    "    print(f\"Strain min:{strains.min(axis=(0,1))}, max:{strains.max(axis=(0,1))}\")\n",
    "    print(f\"Position shape:{positions.shape}, type shape:{particle_types.shape}, strain shape:{strains.shape}\")\n",
    "    print(f\"Unique particle types: {np.unique(particle_types)}\")\n",
    "    \n",
    "    # Data splits: train, valid, test\n",
    "    key = trajectory_name\n",
    "    trajectory_data = (positions, particle_types, strains)\n",
    "\n",
    "    if any(name in trajectory_name for name in val_set):\n",
    "        ds_valid[key] = trajectory_data\n",
    "        valid_info.append(trajectory_name)\n",
    "    elif any(name in trajectory_name for name in test_set):\n",
    "        ds_test[key] = trajectory_data\n",
    "        test_info.append(trajectory_name)\n",
    "    else:\n",
    "        ds_train[key] = trajectory_data\n",
    "        train_info.append(trajectory_name)\n",
    "        \n",
    "    # Extract Vel and Acc statistics\n",
    "    # positions of shape [timestep, particles, dimensions]\n",
    "    vel_trajectory = positions[1:,:,:] - positions[:-1,:,:]\n",
    "    acc_trajectory = vel_trajectory[1:,:,:]- vel_trajectory[:-1,:,:]\n",
    "    \n",
    "    vels = np.concatenate((vels, vel_trajectory.reshape(-1, 2)), axis=0)\n",
    "    accs = np.concatenate((accs, acc_trajectory.reshape(-1, 2)), axis=0)\n",
    "    \n",
    "#print('strain_stats:', strain_stats.mean(), strain_stats.std())\n",
    "\n",
    "vel_mean = list(vels.mean(axis=0))\n",
    "vel_std = list(vels.std(axis=0))\n",
    "acc_mean = list(accs.mean(axis=0))\n",
    "acc_std = list(accs.std(axis=0))\n",
    "\n",
    "# Save the entire dictionary under one key:\n",
    "train_filepath = os.path.join(out_dir, 'train.npz')\n",
    "valid_filepath = os.path.join(out_dir, 'valid.npz')\n",
    "test_filepath = os.path.join(out_dir, 'test.npz')\n",
    "\n",
    "np.savez(train_filepath, trajectories=ds_train)\n",
    "np.savez(valid_filepath, trajectories=ds_valid)\n",
    "np.savez(test_filepath, trajectories=ds_test)\n",
    "\n",
    "print(f\"{len(ds_train)} trajectories parsed and saved to train.npz.\")\n",
    "print(f\"{len(ds_valid)} trajectories parsed and saved to valid.npz.\")\n",
    "print(f\"{len(ds_test)}  trajectories parsed and saved to test.npz.\")\n",
    "\n",
    "\n",
    "# Save meta data\n",
    "out_file = out_dir + 'metadata.json'\n",
    "\n",
    "meta_data = {}\n",
    "meta_data['dim'] = positions.shape[-1]\n",
    "meta_data['sequence_length'] = positions.shape[0]\n",
    "meta_data['dt'] = 0.002 * STEP_SIZE\n",
    "meta_data['bounds'] = [[0, 1], [0, 1]]\n",
    "meta_data['default_connectivity_radius'] = 0.03  # will be reset in train.py\n",
    "meta_data['vel_mean'] = vel_mean\n",
    "meta_data['vel_std'] = vel_std\n",
    "meta_data['acc_mean'] = acc_mean\n",
    "meta_data['acc_std'] = acc_std\n",
    "meta_data['file_train'] = train_info\n",
    "meta_data['file_valid'] = valid_info\n",
    "meta_data['file_test'] = test_info\n",
    "print(meta_data)\n",
    "\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df83234e-88d1-4dc1-b8e3-46ff64a92085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First trajectory key: T-20-100-100.npz\n",
      "Positions shape: (50, 8000, 2)\n",
      "Particle types shape: (8000,)\n",
      "Strains shape: (50, 8000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the npz file\n",
    "data = np.load('/home/jovyan/share/EPIMETHEUS-LOCAL/gns_data/test/train.npz', allow_pickle=True)\n",
    "\n",
    "# Extract the stored dictionary of trajectories\n",
    "ds_train = data['trajectories'].item()\n",
    "\n",
    "# Convert the dictionary items to a list of (key, value) tuples\n",
    "train_data_list = list(ds_train.items())\n",
    "\n",
    "# Get the first trajectory (tuple: key, value)\n",
    "first_key, first_trajectory = train_data_list[0]\n",
    "\n",
    "# Extract positions, particle types, and strains\n",
    "positions, particle_types, strains = first_trajectory  # Unpack the tuple\n",
    "\n",
    "# Print details\n",
    "print(f\"First trajectory key: {first_key}\")\n",
    "print(f\"Positions shape: {positions.shape}\")\n",
    "print(f\"Particle types shape: {particle_types.shape}\")\n",
    "print(f\"Strains shape: {strains.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da8c603-cc64-4174-90be-b581b5d3594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jovyan/share/EPIMETHEUS-LOCAL/gns_data/test/train.npz'\n",
    "data = [item for _, item in np.load(path, allow_pickle=True)['trajectories'].item().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340950cc-817f-4318-959e-e4d5cf19c578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgn",
   "language": "python",
   "name": "cgn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
